{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Train a Neural Network\n",
    "\n",
    "Here we will use Pytorch to build and train a simple neural network step-by-step. Below you will find a sample code, which you will have to finish and execute. But first things first.\n",
    "\n",
    "Let's start with some basic imports:\n",
    "\n",
    "As mentioned during the lesson, we will use a dataset with pet images. We have already build a data loader called `Cifar10Dataloader` so we can simply import `CIFAR10`.\n",
    "\n",
    "Besides the dataset, we also import `torch`, `numpy` and `torchvision` which is a library to help us with data preperation and transformations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import sys\n",
    "cwd = os.getcwd()\n",
    "#add CIFAR10 data in the environment\n",
    "sys.path.append(cwd + '/../cifar10') \n",
    "\n",
    "#Numpy is linear algebra lbrary\n",
    "import numpy as np\n",
    "# Matplotlib is a visualizations library \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import utils\n",
    "from torchvision import transforms\n",
    "#CIFAR10 is a custom Dataloader that loads a subset ofthe data from a local folder\n",
    "from Cifar10Dataloader import CIFAR10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the dataset\n",
    "\n",
    "After loading the images, we have to apply some transformations so we can feed them in the network.  \n",
    "Here we simply convert them to tensors and normalize them. This can be done using `transforms` from `torchvision`.\n",
    "\n",
    "All images fall into 1 of 10 categories of things.\n",
    "\n",
    "Let's also plot some of them to make sure that everything works fine."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch_size=4\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    #convert the images to tensor and normalized them\n",
    "    transform = transforms.Compose([\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    trainset = CIFAR10(root='../cifar10',  transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              shuffle=False, num_workers=1)\n",
    "    return trainloader\n",
    "\n",
    "\n",
    "def show_image(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "       'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(load_data())\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "show_image(utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build and train the model\n",
    "\n",
    "And now for the good stuff. Let's define and train the model. This is where you will have to take action.\n",
    "\n",
    "First you need to define a network that recieves and an input of size 3072, has 3 linear layers with dimensions 128,64,10, and 2 Relu layers in between.\n",
    "\n",
    "Then you'll need to finish the training loop by declaring the  `torch.nn.CrossEntropyLoss` as our loss and `torch.optim.SGD` as our optimizer. Finally, you'll build the training loop and execute 1 or more training epochs. \n",
    "\n",
    "You can find a sample solution in the final cell of this notebook. I will strongly advice you though to spend as much time you need to write the code yourself. It is vital for the remaining of the course to have a solid understanding of how to build models and training loops.\n",
    "\n",
    "* Note that the Jupyter notebook will remain active for 15 mins so you won't be able to run th entire training and see the model converge. But as soon as the running loss is decresing, you should be ok. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## 1. DEFINE MODEL HERE\n",
    "model = nn.Sequential(nn.Linear(3072, 128),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(128, 64),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(64, 10)\n",
    "                           )\n",
    "\n",
    "def train():\n",
    "\n",
    "    training_data = load_data()\n",
    "    \n",
    "    # 2. LOSS AND OPTIMIZER\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    \n",
    "    running_loss = 0.0\n",
    "\n",
    "    for epoch in range(10):\n",
    "        for i, data in enumerate(training_data, 0):\n",
    "           \n",
    "            inputs, labels = data\n",
    "            #reshape images so they can be fed to a nn.Linear()\n",
    "            inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            ##3. RUN BACKPROP\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "           \n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 500 == 499:    # print every 500 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 500))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "    print('Training finished')\n",
    "    \n",
    "\n",
    "train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate\n",
    "\n",
    "After running a few training epochs, you can evaluate the trained model and compute its accuracy on unseen data. To get the prediction, we feed an image to the network and the category that has the bigger output."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate():\n",
    "    dataiter = iter(load_data())\n",
    "    images, labels = dataiter.next()\n",
    "\n",
    "    # print images\n",
    "    show_image(utils.make_grid(images))\n",
    "    print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "    \n",
    "    images = images.view(images.size(0), -1)\n",
    "    outputs = model(images)\n",
    "    \n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))\n",
    "\n",
    "evaluate()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}